% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/partition.loss.R
\name{partition.loss}
\alias{partition.loss}
\alias{binder}
\alias{randi}
\alias{pear}
\alias{arandi}
\alias{VI.lb}
\alias{VI}
\title{Compute Partition Loss or the Expectation of Partition Loss}
\usage{
partition.loss(
  partitions,
  psm = NULL,
  draws = NULL,
  loss = c("binder", "pear", "VI.lb", "VI")[3]
)

binder(partitions, psm)

randi(partition1, partition2)

pear(partitions, psm)

arandi(partition1, partition2)

VI.lb(partitions, psm)

VI(partitions, draws)
}
\arguments{
\item{partitions}{An integer matrix of cluster labels with \eqn{n} columns,
where each row is a partition of \eqn{n} items given as cluster labels. Two
items are in the same subset (i.e., cluster) if their labels are equal.}

\item{psm}{A pairwise similarity matrix, i.e., \eqn{n}-by-\eqn{n} symmetric
matrix whose \eqn{(i,j)} element gives the (estimated) probability that
items \eqn{i} and \eqn{j} are in the same subset (i.e., cluster) of a
partition (i.e., clustering).}

\item{draws}{A \eqn{B}-by-\eqn{n} matrix, where each of the \eqn{B} rows
represents a clustering of \eqn{n} items using cluster labels. For
clustering \eqn{b}, items \eqn{i} and \eqn{j} are in the same cluster if
\code{x[b,i] == x[b,j]}.}

\item{loss}{One of \code{"binder"}, \code{"pear"}, \code{"VI.lb"}, or
\code{"VI"}.}

\item{partition1}{An integer vector of cluster labels for \eqn{n} items. Two
items are in the same subset (i.e., cluster) if their labels are equal.}

\item{partition2}{An integer vector of cluster labels having the same length
as \code{partition1}.}
}
\value{
A numeric vector.
}
\description{
Given two partitions \eqn{\pi*} and \eqn{\pi}, these functions compute the
specified loss when using \eqn{\pi*} to estimate \eqn{\pi}.  Smaller loss
values indicate higher concordance between partitions.  These functions also
compute a Monte Carlo estimate of the expectation for the specified loss
based on samples or a pairwise similarity matrix. The implementation
currently supports the computation of the following criteria: "binder",
"pear", "VI.lb", and "VI".
}
\details{
The partition estimation criteria can be specified using the \code{loss}
argument: \describe{

\item{\code{"binder"}}{Binder loss. Whereas high values of the Rand index
\eqn{R} between \eqn{\pi*} and \eqn{\pi} correspond to high concordance
between these partitions, the Binder loss \eqn{L} for a partition \eqn{\pi*}
in estimating \eqn{\pi} is \eqn{L = n*(n-1)*(1-R)/2}. See also Dahl (2006),
Lau and Green (2007), Dahl and Newton (2007).}

\item{\code{"pear"}}{PEAR loss. Computes with the first-order approximation
of the expectation of the loss associated with the adjusted Rand index
(Hubert and Arabie, 1985).  Whereas high values of the adjusted Rand index
between \eqn{\pi*} and \eqn{\pi} correspond to high concordance between these
partitions, the loss associated with the adjusted Rand index for a partition
\eqn{\pi*} in estimating \eqn{\pi} is one minus the adjusted Rand index
between these partitions.  The adjusted Rand index involves a ratio and the
first-order approximation of the expectation is based on \eqn{E(X/Y) \approx
E(X)/E(Y)}.  See Fritsch and Ickstadt (2009).}

\item{\code{"VI.lb"}}{Lower Bound of the Variation of Information.  Computes
with the lower bound of the expectation of the variation of information loss,
where the lower bound is obtained from Jensen's inequality.  See Wade and
Ghahramani (2018).}

\item{\code{"VI"}}{Variation of Information. Computes with the variation of
information loss. See Meilă (2007), Wade and Ghahramani (2018), and Rastelli
and Friel (2018).}

}

The functions \code{\link{randi}} and \code{\link{arandi}} are convenience
functions. Note that:
\itemize{
\item \code{binder(p1, p2) = choose(length(p1), 2) * ( 1 - randi(p1, p2) )}
\item \code{pear(p1, p2) = 1 - arandi(p1, p2)}
}
}
\examples{
p1 <- iris.clusterings[1,]
p2 <- iris.clusterings[2,]

all.equal(binder(p1, psm(p2)), choose(length(p1), 2) * ( 1 - randi(p1, p2) ))
all.equal(pear(p1, psm(p2)), 1 - arandi(p1, p2))

# For examples, use 'parallel=FALSE' per CRAN rules but, in practice, omit this.
probs <- psm(iris.clusterings, parallel=FALSE)
partitions <- iris.clusterings[1:5,]

partition.loss(partitions, probs, loss="binder")
partition.loss(partitions, probs, loss="pear")
partition.loss(partitions, probs, loss="VI.lb")
partition.loss(partitions, draws=partitions, loss="VI")

all.equal(partition.loss(partitions, probs, loss="binder"), binder(partitions, probs))
all.equal(partition.loss(partitions, probs, loss="pear"), 1 - pear(partitions, probs))
all.equal(partition.loss(partitions, probs, loss="VI.lb"), VI.lb(partitions, probs))
all.equal(partition.loss(partitions, draws=partitions, loss="VI"), VI(partitions, partitions))

}
\references{
W. M. Rand (1971), Objective Criteria for the Evaluation of Clustering
Methods. \emph{Journal of the American Statistical Association}, \bold{66}:
846–850.

D. A. Binder (1978), Bayesian cluster analysis, \emph{Biometrika} \bold{65},
31-38.

L. Hubert and P. Arabie (1985), Comparing Partitions. \emph{Journal of
Classification}, \bold{2}, 193–218.

D. B. Dahl (2006), Model-Based Clustering for Expression Data via a Dirichlet
Process Mixture Model, in \emph{Bayesian Inference for Gene Expression and
Proteomics}, Kim-Anh Do, Peter Müller, Marina Vannucci (Eds.), Cambridge
University Press.

J. W. Lau and P. J. Green (2007), Bayesian model based clustering procedures,
\emph{Journal of Computational and Graphical Statistics} \bold{16}, 526-558.

M. Meilă (2007), Comparing Clusterings - an Information Based Distance.
\emph{Journal of Multivariate Analysis}, \bold{98}: 873–895.

D. B. Dahl and M. A. Newton (2007), Multiple Hypothesis Testing by Clustering
Treatment Effects, \emph{Journal of the American Statistical Association},
\bold{102}, 517-526.

A. Fritsch and K. Ickstadt (2009), An improved criterion for clustering based
on the posterior similarity matrix, \emph{Bayesian Analysis}, \bold{4},
367-391.

S. Wade and Z. Ghahramani (2018), Bayesian cluster analysis: Point estimation
and credible balls. \emph{Bayesian Analysis}, \bold{13:2}, 559-626.

R. Rastelli and N. Friel (2018), Optimal Bayesian estimators for latent
variable cluster models. \emph{Statistics and Computing}, \bold{28},
1169-1186.
}
\seealso{
\code{\link{psm}}, \code{\link{salso}}, \code{\link{dlso}}
}
